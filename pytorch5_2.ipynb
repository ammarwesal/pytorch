{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7xAXr9qwZdAL0AAxY6Rgv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarwesal/pytorch/blob/main/pytorch5_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###prediction:torch\n",
        "###gradienT_computation:Autograd\n",
        "###loss_computation:pytorch_loss\n",
        "###parameter_updates:pytorch_optimizer"
      ],
      "metadata": {
        "id": "vr8_LSNK3-68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87HCie1q3vWE",
        "outputId": "80d6c369-1d9d-46d6-88cb-40d59ef49940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "prediction before training: f(5)=4.717\n",
            "epoch 1: w=0.907, loss=6.73650455\n",
            "prediction after training: f(5)=5.465\n",
            "epoch 2: w=1.025, loss=4.74286985\n",
            "prediction after training: f(5)=6.089\n",
            "epoch 3: w=1.123, loss=3.35911918\n",
            "prediction after training: f(5)=6.608\n",
            "epoch 4: w=1.205, loss=2.39855576\n",
            "prediction after training: f(5)=7.042\n",
            "epoch 5: w=1.273, loss=1.73163652\n",
            "prediction after training: f(5)=7.403\n",
            "epoch 6: w=1.330, loss=1.26847243\n",
            "prediction after training: f(5)=7.704\n",
            "epoch 7: w=1.378, loss=0.94669294\n",
            "prediction after training: f(5)=7.956\n",
            "epoch 8: w=1.418, loss=0.72301924\n",
            "prediction after training: f(5)=8.165\n",
            "epoch 9: w=1.451, loss=0.56742102\n",
            "prediction after training: f(5)=8.341\n",
            "epoch 10: w=1.480, loss=0.45906249\n",
            "prediction after training: f(5)=8.487\n",
            "epoch 11: w=1.503, loss=0.38348359\n",
            "prediction after training: f(5)=8.609\n",
            "epoch 12: w=1.523, loss=0.33065248\n",
            "prediction after training: f(5)=8.711\n",
            "epoch 13: w=1.540, loss=0.29360831\n",
            "prediction after training: f(5)=8.797\n",
            "epoch 14: w=1.554, loss=0.26752037\n",
            "prediction after training: f(5)=8.869\n",
            "epoch 15: w=1.566, loss=0.24903704\n",
            "prediction after training: f(5)=8.929\n",
            "epoch 16: w=1.576, loss=0.23583281\n",
            "prediction after training: f(5)=8.979\n",
            "epoch 17: w=1.585, loss=0.22629389\n",
            "prediction after training: f(5)=9.022\n",
            "epoch 18: w=1.592, loss=0.21930042\n",
            "prediction after training: f(5)=9.057\n",
            "epoch 19: w=1.598, loss=0.21407537\n",
            "prediction after training: f(5)=9.087\n",
            "epoch 20: w=1.604, loss=0.21007967\n",
            "prediction after training: f(5)=9.113\n",
            "epoch 21: w=1.609, loss=0.20693937\n",
            "prediction after training: f(5)=9.134\n",
            "epoch 22: w=1.613, loss=0.20439442\n",
            "prediction after training: f(5)=9.153\n",
            "epoch 23: w=1.616, loss=0.20226520\n",
            "prediction after training: f(5)=9.169\n",
            "epoch 24: w=1.619, loss=0.20042631\n",
            "prediction after training: f(5)=9.182\n",
            "epoch 25: w=1.622, loss=0.19879121\n",
            "prediction after training: f(5)=9.194\n",
            "epoch 26: w=1.625, loss=0.19729963\n",
            "prediction after training: f(5)=9.204\n",
            "epoch 27: w=1.627, loss=0.19590969\n",
            "prediction after training: f(5)=9.212\n",
            "epoch 28: w=1.629, loss=0.19459240\n",
            "prediction after training: f(5)=9.220\n",
            "epoch 29: w=1.631, loss=0.19332762\n",
            "prediction after training: f(5)=9.227\n",
            "epoch 30: w=1.633, loss=0.19210166\n",
            "prediction after training: f(5)=9.233\n",
            "epoch 31: w=1.635, loss=0.19090426\n",
            "prediction after training: f(5)=9.238\n",
            "epoch 32: w=1.636, loss=0.18972905\n",
            "prediction after training: f(5)=9.243\n",
            "epoch 33: w=1.638, loss=0.18857121\n",
            "prediction after training: f(5)=9.247\n",
            "epoch 34: w=1.639, loss=0.18742743\n",
            "prediction after training: f(5)=9.251\n",
            "epoch 35: w=1.640, loss=0.18629557\n",
            "prediction after training: f(5)=9.255\n",
            "epoch 36: w=1.642, loss=0.18517393\n",
            "prediction after training: f(5)=9.258\n",
            "epoch 37: w=1.643, loss=0.18406126\n",
            "prediction after training: f(5)=9.262\n",
            "epoch 38: w=1.644, loss=0.18295702\n",
            "prediction after training: f(5)=9.265\n",
            "epoch 39: w=1.645, loss=0.18186055\n",
            "prediction after training: f(5)=9.268\n",
            "epoch 40: w=1.647, loss=0.18077138\n",
            "prediction after training: f(5)=9.270\n",
            "epoch 41: w=1.648, loss=0.17968932\n",
            "prediction after training: f(5)=9.273\n",
            "epoch 42: w=1.649, loss=0.17861411\n",
            "prediction after training: f(5)=9.276\n",
            "epoch 43: w=1.650, loss=0.17754568\n",
            "prediction after training: f(5)=9.278\n",
            "epoch 44: w=1.651, loss=0.17648380\n",
            "prediction after training: f(5)=9.281\n",
            "epoch 45: w=1.652, loss=0.17542838\n",
            "prediction after training: f(5)=9.283\n",
            "epoch 46: w=1.653, loss=0.17437918\n",
            "prediction after training: f(5)=9.285\n",
            "epoch 47: w=1.654, loss=0.17333652\n",
            "prediction after training: f(5)=9.288\n",
            "epoch 48: w=1.655, loss=0.17230004\n",
            "prediction after training: f(5)=9.290\n",
            "epoch 49: w=1.656, loss=0.17126976\n",
            "prediction after training: f(5)=9.292\n",
            "epoch 50: w=1.658, loss=0.17024577\n",
            "prediction after training: f(5)=9.294\n",
            "epoch 51: w=1.659, loss=0.16922782\n",
            "prediction after training: f(5)=9.296\n",
            "epoch 52: w=1.660, loss=0.16821611\n",
            "prediction after training: f(5)=9.299\n",
            "epoch 53: w=1.661, loss=0.16721030\n",
            "prediction after training: f(5)=9.301\n",
            "epoch 54: w=1.662, loss=0.16621056\n",
            "prediction after training: f(5)=9.303\n",
            "epoch 55: w=1.663, loss=0.16521679\n",
            "prediction after training: f(5)=9.305\n",
            "epoch 56: w=1.664, loss=0.16422901\n",
            "prediction after training: f(5)=9.307\n",
            "epoch 57: w=1.665, loss=0.16324717\n",
            "prediction after training: f(5)=9.309\n",
            "epoch 58: w=1.666, loss=0.16227113\n",
            "prediction after training: f(5)=9.311\n",
            "epoch 59: w=1.667, loss=0.16130090\n",
            "prediction after training: f(5)=9.313\n",
            "epoch 60: w=1.668, loss=0.16033649\n",
            "prediction after training: f(5)=9.316\n",
            "epoch 61: w=1.669, loss=0.15937783\n",
            "prediction after training: f(5)=9.318\n",
            "epoch 62: w=1.670, loss=0.15842499\n",
            "prediction after training: f(5)=9.320\n",
            "epoch 63: w=1.671, loss=0.15747775\n",
            "prediction after training: f(5)=9.322\n",
            "epoch 64: w=1.672, loss=0.15653631\n",
            "prediction after training: f(5)=9.324\n",
            "epoch 65: w=1.673, loss=0.15560040\n",
            "prediction after training: f(5)=9.326\n",
            "epoch 66: w=1.674, loss=0.15467009\n",
            "prediction after training: f(5)=9.328\n",
            "epoch 67: w=1.675, loss=0.15374528\n",
            "prediction after training: f(5)=9.330\n",
            "epoch 68: w=1.676, loss=0.15282613\n",
            "prediction after training: f(5)=9.332\n",
            "epoch 69: w=1.677, loss=0.15191235\n",
            "prediction after training: f(5)=9.334\n",
            "epoch 70: w=1.678, loss=0.15100414\n",
            "prediction after training: f(5)=9.336\n",
            "epoch 71: w=1.679, loss=0.15010127\n",
            "prediction after training: f(5)=9.338\n",
            "epoch 72: w=1.679, loss=0.14920378\n",
            "prediction after training: f(5)=9.340\n",
            "epoch 73: w=1.680, loss=0.14831167\n",
            "prediction after training: f(5)=9.342\n",
            "epoch 74: w=1.681, loss=0.14742509\n",
            "prediction after training: f(5)=9.344\n",
            "epoch 75: w=1.682, loss=0.14654356\n",
            "prediction after training: f(5)=9.346\n",
            "epoch 76: w=1.683, loss=0.14566745\n",
            "prediction after training: f(5)=9.348\n",
            "epoch 77: w=1.684, loss=0.14479646\n",
            "prediction after training: f(5)=9.350\n",
            "epoch 78: w=1.685, loss=0.14393076\n",
            "prediction after training: f(5)=9.352\n",
            "epoch 79: w=1.686, loss=0.14307022\n",
            "prediction after training: f(5)=9.353\n",
            "epoch 80: w=1.687, loss=0.14221479\n",
            "prediction after training: f(5)=9.355\n",
            "epoch 81: w=1.688, loss=0.14136454\n",
            "prediction after training: f(5)=9.357\n",
            "epoch 82: w=1.689, loss=0.14051934\n",
            "prediction after training: f(5)=9.359\n",
            "epoch 83: w=1.690, loss=0.13967922\n",
            "prediction after training: f(5)=9.361\n",
            "epoch 84: w=1.691, loss=0.13884409\n",
            "prediction after training: f(5)=9.363\n",
            "epoch 85: w=1.692, loss=0.13801399\n",
            "prediction after training: f(5)=9.365\n",
            "epoch 86: w=1.693, loss=0.13718885\n",
            "prediction after training: f(5)=9.367\n",
            "epoch 87: w=1.694, loss=0.13636854\n",
            "prediction after training: f(5)=9.369\n",
            "epoch 88: w=1.694, loss=0.13555323\n",
            "prediction after training: f(5)=9.371\n",
            "epoch 89: w=1.695, loss=0.13474283\n",
            "prediction after training: f(5)=9.373\n",
            "epoch 90: w=1.696, loss=0.13393724\n",
            "prediction after training: f(5)=9.374\n",
            "epoch 91: w=1.697, loss=0.13313641\n",
            "prediction after training: f(5)=9.376\n",
            "epoch 92: w=1.698, loss=0.13234037\n",
            "prediction after training: f(5)=9.378\n",
            "epoch 93: w=1.699, loss=0.13154921\n",
            "prediction after training: f(5)=9.380\n",
            "epoch 94: w=1.700, loss=0.13076268\n",
            "prediction after training: f(5)=9.382\n",
            "epoch 95: w=1.701, loss=0.12998082\n",
            "prediction after training: f(5)=9.384\n",
            "epoch 96: w=1.702, loss=0.12920377\n",
            "prediction after training: f(5)=9.386\n",
            "epoch 97: w=1.703, loss=0.12843119\n",
            "prediction after training: f(5)=9.387\n",
            "epoch 98: w=1.704, loss=0.12766337\n",
            "prediction after training: f(5)=9.389\n",
            "epoch 99: w=1.704, loss=0.12690002\n",
            "prediction after training: f(5)=9.391\n",
            "epoch 100: w=1.705, loss=0.12614129\n",
            "prediction after training: f(5)=9.393\n"
          ]
        }
      ],
      "source": [
        "#1) design model(input,output size, forward pass)\n",
        "#2) construct loss and optimizer\n",
        "#3) Training loop\n",
        "# -forward pass:compute predictions\n",
        "# -backward pass:gradients\n",
        "# -update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#f=w*x\n",
        "\n",
        "#f=2*x\n",
        "X=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
        "y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
        "\n",
        "X_test=torch.tensor([5],dtype=torch.float32)\n",
        "\n",
        "n_samples,n_features=X.shape\n",
        "print(n_samples,n_features)\n",
        "\n",
        "input_size=n_features\n",
        "output_size=n_features\n",
        "\n",
        "#model=nn.Linear(input_size,output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "    #define layers\n",
        "    self.lin=nn.Linear(input_dim,output_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model=LinearRegression(input_size,output_size)\n",
        "\n",
        "print(f'prediction before training: f(5)={model(X_test).item():.3f}')\n",
        "\n",
        "#training\n",
        "learning_rate=0.01\n",
        "n_iters=100\n",
        "\n",
        "loss=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction=forward pass\n",
        "  y_pred=model(X)\n",
        "\n",
        "  #loss\n",
        "  l=loss(y,y_pred)\n",
        "\n",
        "  #gradient=backward pass\n",
        "  l.backward()#dL/dw\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch%1==0:\n",
        "    [w,b]=model.parameters()\n",
        "    print(f'epoch {epoch+1}: w={w[0][0].item():.3f}, loss={l:.8f}')\n",
        "\n",
        "  print(f'prediction after training: f(5)={model(X_test).item():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EcOSejrB52z5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}